{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'MyAttention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d37ee9737099>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_from_yaml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mMyAttention\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mattention\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'MyAttention'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import multiprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.utils import shuffle \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml\n",
    "from Attention import attention\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters(word2vec)\n",
    "vocab_dim = 200\n",
    "maxlen = 100\n",
    "n_iterations = 1  # ideally more..\n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/train_first.csv')\n",
    "#data=data.drop(['Id'],axis=1)\n",
    "#data.head()\n",
    "data=data.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "#划分x  y\n",
    "X=data['Discuss']\n",
    "Y=data['Score']\n",
    " \n",
    "#y ONEHOT\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit([1,2,3,4,5])\n",
    "Y=lb.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载停用词\n",
    "def get_stopwords(path):\n",
    "    return [line.strip() for line in open(path,'r',encoding='utf-8').readlines()]\n",
    "#句子去停用词\n",
    "def removestopwords(sentence):\n",
    "        stopwords_list=get_stopwords('data/stopwords.txt')\n",
    "        outstr=[]\n",
    "        for word in sentence:\n",
    "            if not word in stopwords_list:\n",
    "                if word!='\\n' and word!='\\t':\n",
    "                     outstr.append(word)\n",
    "        return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词 并去掉停用词\n",
    "def cut(sentence):\n",
    "    return removestopwords(jieba.cut(sentence))\n",
    "#分词后的word\n",
    "cabs=[cut(x) for x in X]\n",
    "#cabs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def word2vec_train(combined):\n",
    "\n",
    "    model = Word2Vec(size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     iter=n_iterations)\n",
    "    model.build_vocab(combined)\n",
    "    model.train(combined,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    model.save('data/Word2vec.pkl')\n",
    "    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n",
    "    return   index_dict, word_vectors,combined\n",
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.vocab.keys(),\n",
    "                            allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print ('No data provided...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据词向量重新填充x\n",
    "index_dict, word_vectors,combined=word2vec_train(cabs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#搭建网络结构\n",
    "input_x=tf.placeholder(shape=[None,None],dtype=tf.int32,name='input_x')\n",
    "label=tf.placeholder(shape=[None,None],dtype=tf.int32,name='target')\n",
    "lr=tf.placeholder(dtype=tf.float32,name='learning_rate')\n",
    "#sequence_lengths=tf.placeholder(shape=[None],dtype=tf.int32,name='sequence_lengths')\n",
    "keep_prob=tf.placeholder(dtype=tf.float32,name='keep_prob')\n",
    "#embedding_placeholder = tf.placeholder(tf.float32, [None, None],name='embedding_placeholder')\n",
    "#super params\n",
    "hidden_dim=128\n",
    "layer_num=2\n",
    "attention_size=100\n",
    "\n",
    "#bilstm\n",
    "def get_lstm_dropout():\n",
    "    lstm_cell=tf.contrib.rnn.BasicLSTMCell(hidden_dim,forget_bias=1.0)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell=lstm_cell,output_keep_prob=keep_prob)\n",
    "#fw\n",
    "fw_cells=tf.contrib.rnn.MultiRNNCell([get_lstm_dropout() for _ in range(layer_num)])\n",
    "initial_state_fw =fw_cells.zero_state(tf.shape(input_x)[0],tf.float32)\n",
    "#bw\n",
    "bw_cells=tf.contrib.rnn.MultiRNNCell([get_lstm_dropout() for _ in range(layer_num)])\n",
    "initial_state_bw=bw_cells.zero_state(tf.shape(input_x)[0],tf.float32)\n",
    "#embedding\n",
    "embedding = tf.Variable(tf.random_uniform((len(index_dict)+1,vocab_dim), -1, 1),name='embedding')\n",
    "embed = tf.nn.embedding_lookup(embedding, input_x)\n",
    "#\n",
    "outputs,_,_=tf.nn.bidirectional_dynamic_rnn(fw_cells,bw_cells,embed,initial_state_fw=initial_state_fw,\n",
    "    initial_state_bw=initial_state_bw,dtype=tf.float32)\n",
    "\n",
    "#attention mechanism\n",
    "attention_output,alphas= attention(outputs, attention_size, return_alphas=True)\n",
    "\n",
    "#with tf.variable_scope('optmizer'):\n",
    "predictions=tf.contrib.layers.fully_connected(attention_output[:,-1],5,activation_fn=tf.nn.softmax)\n",
    "cost=tf.losses.mean_squared_error(label,predictions)\n",
    "optimizer=tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "#validation accuracy\n",
    "correct_pred=tf.equal(tf.cast(tf.round(predictions),tf.int32),label)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batching\n",
    "def get_batching(x,y,batch_size=100):\n",
    "    n_batches=len(x)//batch_size\n",
    "    x,y=x[:n_batches*batch_size],y[:n_batches*batch_size]\n",
    "    for ii in range(0,len(x),batch_size):\n",
    "        yield x[ii:ii+batch_size],y[ii:ii+batch_size]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "#super params\n",
    "epochs=10\n",
    "batch_size=100\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session()as sess:\n",
    "    writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_train, x_test, y_train, y_test=train_test_split(combined, Y, test_size=0.2)#get_data(index_dict,word_vectors,combined,Y)\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        for ii, (x, y) in enumerate(get_batching(x_train, y_train, batch_size), 1):\n",
    "            feed={input_x:x,label:y,lr:0.001,keep_prob:0.5}\n",
    "            loss,_=sess.run([cost,optimizer],feed_dict=feed)\n",
    "            tf.summary.scalar('loss',loss)\n",
    "            if iteration%50==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "            if iteration%100==0:\n",
    "                val_acc = []\n",
    "                for x, y in get_batching(x_test, y_test, batch_size):\n",
    "                    feed = {input_x: x,\n",
    "                            label: y,                            \n",
    "                            keep_prob: 1}\n",
    "                    batch_acc= sess.run(accuracy, feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                tf.summary.scalar('accuracy',np.mean(val_acc))\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data=pd.read_csv('data/predict_first.csv')\n",
    "cabs=[cut(x) for x in predict_data['Discuss']]\n",
    "_,_,predict_X=word2vec_train(cabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.train.Saver()\n",
    "model_file=tf.train.latest_checkpoint('checkpoints/')\n",
    "with tf.Session()as sess:\n",
    "    #sess.run(tf.global_variables_initializer()) \n",
    "    saver.restore(sess,model_file)\n",
    "    predict_arr=[]\n",
    "    for ii in range(0,len(predict_X),100):\n",
    "        for p in sess.run(predictions,feed_dict={input_x:predict_X[ii:ii+100],keep_prob:1.0}):\n",
    "            predict_arr.append(np.argmax(p)+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
