{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import multiprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters(word2vec)\n",
    "vocab_dim = 200\n",
    "maxlen = 100\n",
    "n_iterations = 1  # ideally more..\n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/train_first.csv')\n",
    "#data=data.drop(['Id'],axis=1)\n",
    "#data.head()\n",
    "#划分x  y\n",
    "X=data['Discuss']\n",
    "Y=data['Score']\n",
    "\n",
    "#y ONEHOT\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit([1, 2,3,4,5])\n",
    "Y=lb.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载停用词\n",
    "def get_stopwords(path):\n",
    "    return [line.strip() for line in open(path,'r',encoding='utf-8').readlines()]\n",
    "#句子去停用词\n",
    "def removestopwords(sentence):\n",
    "        stopwords_list=get_stopwords('data/stopwords.txt')\n",
    "        outstr=[]\n",
    "        for word in sentence:\n",
    "            if not word in stopwords_list:\n",
    "                if word!='\\n' and word!='\\t':\n",
    "                     outstr.append(word)\n",
    "        return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhanggd\\AppData\\Local\\Temp\\5\\jieba.cache\n",
      "Loading model cost 2.337 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#分词 并去掉停用词\n",
    "def cut(sentence):\n",
    "    return removestopwords(jieba.cut(sentence))\n",
    "#分词后的word\n",
    "cabs=[cut(x) for x in X]\n",
    "#cabs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def word2vec_train(combined):\n",
    "\n",
    "    model = Word2Vec(size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     iter=n_iterations)\n",
    "    model.build_vocab(combined)\n",
    "    model.train(combined,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    model.save('data/Word2vec.pkl')\n",
    "    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n",
    "    return   index_dict, word_vectors,combined\n",
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.vocab.keys(),\n",
    "                            allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print ('No data provided...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n",
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "#根据词向量重新填充x\n",
    "index_dict, word_vectors,combined=word2vec_train(cabs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getdata\n",
    "def get_data(index_dict,word_vectors,combined,y):\n",
    "    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))#索引为0的词语，词向量全为0\n",
    "    for word, index in index_dict.items():#从索引为1的词语开始，对每个词语对应其词向量\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "    print (x_train.shape,y_train.shape)\n",
    "    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建网络结构\n",
    "input_x=tf.placeholder(shape=[None,None],dtype=tf.int32,name='input_x')\n",
    "label=tf.placeholder(shape=[None,None],dtype=tf.int32,name='target')\n",
    "lr=tf.placeholder(dtype=tf.float32,name='learning_rate')\n",
    "#sequence_lengths=tf.placeholder(shape=[None],dtype=tf.int32,name='sequence_lengths')\n",
    "keep_prob=tf.placeholder(dtype=tf.float32,name='keep_prob')\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [None, None],name='embedding_placeholder')\n",
    "#super params\n",
    "hidden_dim=128\n",
    "batch_size=100\n",
    "layer_num=2\n",
    "#bilstm\n",
    "# with tf.variable_scope('bi_lstm'):\n",
    "#     #lstm层\n",
    "#     lstm_fw_cell=rnn.BasicLSTMCell(hidden_dim,forget_bias=1.0,state_is_tuple=True)\n",
    "#     lstm_bw_cell=rnn.BasicLSTMCell(hidden_dim,forget_bias=1.0,state_is_tuple=True)\n",
    "#     #dropout\n",
    "#     lstm_fw_cell=rnn.DropoutWrapper(cell=lstm_fw_cell,input_keep_prob=1.0,out_keep_prob=keep_prob)\n",
    "#     lstm_bw_cell=rnn.DropoutWrapper(cell=lstm_bw_cell,input_keep_prob=1.0,out_keep_prob=keep_prob)\n",
    "#     #多层lstm\n",
    "#     cell_fw=rnn.MultiRNNCell([lstm_fw_cell]*layer_num, state_is_tuple=True)\n",
    "#     cell_bw=rnn.MultiRNNCell([lstm_fw_cell]*layer_num, state_is_tuple=True)\n",
    "#     #初始状态\n",
    "#     initial_state_fw=cell_fw.zero_state(batch_size,tf.float32)\n",
    "#     initial_state_bw=cell_bw.zero_state(batch_size,tf.float32)\n",
    "\n",
    "\n",
    "#with tf.variable_scope('embedding'):\n",
    "embedding = tf.Variable(tf.constant(0.0, shape=[len(index_dict)+1,vocab_dim]),trainable=False, name=\"embedding\")\n",
    "tf.assign(embedding,embedding_placeholder)\n",
    "embed = tf.nn.embedding_lookup(embedding, input_x)\n",
    "#lstm\n",
    "#with tf.variable_scope('lstm'):\n",
    "\n",
    "def get_lstm_dropout():\n",
    "    lstm_cell=tf.contrib.rnn.BasicLSTMCell(hidden_dim)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell=lstm_cell,output_keep_prob=keep_prob)\n",
    "cell=tf.contrib.rnn.MultiRNNCell([get_lstm_dropout() for _ in range(layer_num)] )\n",
    "initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "outputs, final_state=tf.nn.dynamic_rnn(cell,embed,initial_state=initial_state)\n",
    "\n",
    "#with tf.variable_scope('optmizer'):\n",
    "predictions=tf.contrib.layers.fully_connected(outputs[:,-1],5,activation_fn=tf.nn.softmax)\n",
    "cost=tf.losses.mean_squared_error(label,predictions)\n",
    "optimizer=tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "#validation accuracy\n",
    "correct_pred=tf.equal(tf.cast(tf.round(predictions),tf.int32),label)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batching\n",
    "def get_batching(x,y,batch_size=100):\n",
    "    n_batches=len(x)//batch_size\n",
    "    x,y=x[:n_batches*batch_size],y[:n_batches*batch_size]\n",
    "    for ii in range(0,len(x),batch_size):\n",
    "        yield x[ii:ii+batch_size],y[ii:ii+batch_size]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "#super params\n",
    "epochs=10\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session()as sess:\n",
    "    #writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict,word_vectors,combined,Y)\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        for ii, (x, y) in enumerate(get_batching(x_train, y_train, batch_size), 1):\n",
    "            feed={input_x:x,label:y,lr:0.001,keep_prob:0.2,embedding_placeholder:embedding_weights}\n",
    "            loss,_=sess.run([cost,optimizer],feed_dict=feed)\n",
    "            #tf.summary.scalar('loss',loss)\n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                for x, y in get_batches(x_test, y_test, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y,                            \n",
    "                            keep_prob: 1}\n",
    "                    batch_acc= sess.run(accuracy, feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                #tf.summary.scalar('accuracy',np.mean(val_acc))\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
