{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters(word2vec)\n",
    "vocab_dim = 200\n",
    "maxlen = 100\n",
    "n_iterations = 1  # ideally more..\n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/train_first.csv')\n",
    "#data=data.drop(['Id'],axis=1)\n",
    "#data.head()\n",
    "#划分x  y\n",
    "X=data['Discuss']\n",
    "Y=data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载停用词\n",
    "def get_stopwords(path):\n",
    "    return [line.strip() for line in open(path,'r',encoding='utf-8').readlines()]\n",
    "#句子去停用词\n",
    "def removestopwords(sentence):\n",
    "        stopwords_list=get_stopwords('data/stopwords.txt')\n",
    "        outstr=[]\n",
    "        for word in sentence:\n",
    "            if not word in stopwords_list:\n",
    "                if word!='\\n' and word!='\\t':\n",
    "                     outstr.append(word)\n",
    "        return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ZHANGG~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.111 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#分词 并去掉停用词\n",
    "def cut(sentence):\n",
    "    return removestopwords(jieba.cut(sentence))\n",
    "#分词后的word\n",
    "cabs=[cut(x) for x in X]\n",
    "#cabs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def word2vec_train(combined):\n",
    "\n",
    "    model = Word2Vec(size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     iter=n_iterations)\n",
    "    model.build_vocab(combined)\n",
    "    model.train(combined,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    model.save('data/Word2vec.pkl')\n",
    "    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n",
    "    return   index_dict, word_vectors,combined\n",
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.vocab.keys(),\n",
    "                            allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print ('No data provided...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "#根据词向量重新填充x\n",
    "index_dict, word_vectors,combined=word2vec_train(cabs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#搭建网络结构\n",
    "input_x=tf.placeholder(shape=[None,None,None],dtype=tf.float32,name='input_x')\n",
    "lable=tf.placeholder(shape=[None,None],dtype=tf.int32,name='target')\n",
    "lr=tf.placeholder(dtype=tf.float32,name='learning_rate')\n",
    "sequence_lengths=tf.placeholder(shape=[None],dtype=tf.int32,name='sequence_lengths')\n",
    "keep_prob=tf.placeholder(dtype=tf.float32,name='keep_prob')\n",
    "#super params\n",
    "hidden_dim=128\n",
    "batch_size=100\n",
    "layer_num=2\n",
    "#bilstm\n",
    "# with tf.variable_scope('bi_lstm'):\n",
    "#     #lstm层\n",
    "#     lstm_fw_cell=rnn.BasicLSTMCell(hidden_dim,forget_bias=1.0,state_is_tuple=True)\n",
    "#     lstm_bw_cell=rnn.BasicLSTMCell(hidden_dim,forget_bias=1.0,state_is_tuple=True)\n",
    "#     #dropout\n",
    "#     lstm_fw_cell=rnn.DropoutWrapper(cell=lstm_fw_cell,input_keep_prob=1.0,out_keep_prob=keep_prob)\n",
    "#     lstm_bw_cell=rnn.DropoutWrapper(cell=lstm_bw_cell,input_keep_prob=1.0,out_keep_prob=keep_prob)\n",
    "#     #多层lstm\n",
    "#     cell_fw=rnn.MultiRNNCell([lstm_fw_cell]*layer_num, state_is_tuple=True)\n",
    "#     cell_bw=rnn.MultiRNNCell([lstm_fw_cell]*layer_num, state_is_tuple=True)\n",
    "#     #初始状态\n",
    "#     initial_state_fw=cell_fw.zero_state(batch_size,tf.float32)\n",
    "#     initial_state_bw=cell_bw.zero_state(batch_size,tf.float32)\n",
    "\n",
    "#lstm\n",
    "with tf.variable_scope('lstm'):\n",
    "    lstm_cell=tf.contrib.rnn.BasicLSTMCell(hidden_dim)\n",
    "    def get_lstm_dropout():\n",
    "        return tf.contrib.rnn.DropoutWrapper(cell=lstm_cell,output_keep_prob=keep_prob)\n",
    "    cell=tf.contrib.rnn.MultiRNNCell([get_lstm_dropout()] for _ in range(layer_num))\n",
    "    initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "    outputs, final_state=tf.nn.dynamic_rnn(cell,input_x,sequence_length=sequence_lengths,initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#预测"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
