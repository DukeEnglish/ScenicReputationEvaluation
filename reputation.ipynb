{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import multiprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.utils import shuffle \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters(word2vec)\n",
    "vocab_dim = 200\n",
    "maxlen = 100\n",
    "n_iterations = 1  # ideally more..\n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/train_first.csv')\n",
    "#data=data.drop(['Id'],axis=1)\n",
    "#data.head()\n",
    "data=data.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "#划分x  y\n",
    "X=data['Discuss']\n",
    "Y=data['Score']\n",
    " \n",
    "#y ONEHOT\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit([1,2,3,4,5])\n",
    "Y=lb.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'了之后才知道为啥这个景区那么有名，一定要先了解下历史。可以说，有了这个都江堰，才有后来土地如此肥沃的天府之国，四川人民都应该感谢李冰。工程比较宏伟，门票价格比较贵100元每人。夏天去站在桥上比较凉快，挨着古城，可以去买吃买喝，价格不贵！节假日人很多！停车10元'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载停用词\n",
    "def get_stopwords(path):\n",
    "    return [line.strip() for line in open(path,'r',encoding='utf-8').readlines()]\n",
    "#句子去停用词\n",
    "def removestopwords(sentence):\n",
    "        stopwords_list=get_stopwords('data/stopwords.txt')\n",
    "        outstr=[]\n",
    "        for word in sentence:\n",
    "            if not word in stopwords_list:\n",
    "                if word!='\\n' and word!='\\t':\n",
    "                     outstr.append(word)\n",
    "        return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhanggd\\AppData\\Local\\Temp\\5\\jieba.cache\n",
      "Loading model cost 3.049 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#分词 并去掉停用词\n",
    "def cut(sentence):\n",
    "    return removestopwords(jieba.cut(sentence))\n",
    "#分词后的word\n",
    "cabs=[cut(x) for x in X]\n",
    "#cabs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def word2vec_train(combined):\n",
    "\n",
    "    model = Word2Vec(size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     iter=n_iterations)\n",
    "    model.build_vocab(combined)\n",
    "    model.train(combined,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    model.save('data/Word2vec.pkl')\n",
    "    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n",
    "    return   index_dict, word_vectors,combined\n",
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.vocab.keys(),\n",
    "                            allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print ('No data provided...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n",
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "#根据词向量重新填充x\n",
    "index_dict, word_vectors,combined=word2vec_train(cabs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getdata\n",
    "# def get_data(index_dict,word_vectors,combined,y):\n",
    "#     n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "#     embedding_weights = np.zeros((n_symbols, vocab_dim))#索引为0的词语，词向量全为0\n",
    "#     for word, index in index_dict.items():#从索引为1的词语开始，对每个词语对应其词向量\n",
    "#         embedding_weights[index, :] = word_vectors[word]\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "#     print (x_train.shape,y_train.shape)\n",
    "#     return n_symbols,embedding_weights,x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#搭建网络结构\n",
    "input_x=tf.placeholder(shape=[None,None],dtype=tf.int32,name='input_x')\n",
    "label=tf.placeholder(shape=[None,None],dtype=tf.int32,name='target')\n",
    "lr=tf.placeholder(dtype=tf.float32,name='learning_rate')\n",
    "#sequence_lengths=tf.placeholder(shape=[None],dtype=tf.int32,name='sequence_lengths')\n",
    "keep_prob=tf.placeholder(dtype=tf.float32,name='keep_prob')\n",
    "#embedding_placeholder = tf.placeholder(tf.float32, [None, None],name='embedding_placeholder')\n",
    "#super params\n",
    "hidden_dim=128\n",
    "batch_size=100\n",
    "layer_num=2\n",
    "#bilstm\n",
    "# with tf.variable_scope('bi_lstm'):\n",
    "#     #lstm层\n",
    "#     lstm_fw_cell=rnn.BasicLSTMCell(hidden_dim,forget_bias=1.0,state_is_tuple=True)\n",
    "#     lstm_bw_cell=rnn.BasicLSTMCell(hidden_dim,forget_bias=1.0,state_is_tuple=True)\n",
    "#     #dropout\n",
    "#     lstm_fw_cell=rnn.DropoutWrapper(cell=lstm_fw_cell,input_keep_prob=1.0,out_keep_prob=keep_prob)\n",
    "#     lstm_bw_cell=rnn.DropoutWrapper(cell=lstm_bw_cell,input_keep_prob=1.0,out_keep_prob=keep_prob)\n",
    "#     #多层lstm\n",
    "#     cell_fw=rnn.MultiRNNCell([lstm_fw_cell]*layer_num, state_is_tuple=True)\n",
    "#     cell_bw=rnn.MultiRNNCell([lstm_fw_cell]*layer_num, state_is_tuple=True)\n",
    "#     #初始状态\n",
    "#     initial_state_fw=cell_fw.zero_state(batch_size,tf.float32)\n",
    "#     initial_state_bw=cell_bw.zero_state(batch_size,tf.float32)\n",
    "\n",
    "\n",
    "#with tf.variable_scope('embedding'):\n",
    "#embedding = tf.Variable(tf.constant(0.0, shape=[len(index_dict)+1,vocab_dim]),trainable=True, name=\"embedding\")\n",
    "embedding = tf.Variable(tf.random_uniform((len(index_dict)+1,vocab_dim), -1, 1),name='embedding')\n",
    "#tf.assign(embedding,embedding_placeholder)\n",
    "#print(embedding)\n",
    "embed = tf.nn.embedding_lookup(embedding, input_x)\n",
    "#lstm\n",
    "#with tf.variable_scope('lstm'):\n",
    "\n",
    "def get_lstm_dropout():\n",
    "    lstm_cell=tf.contrib.rnn.BasicLSTMCell(hidden_dim)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell=lstm_cell,output_keep_prob=keep_prob)\n",
    "cell=tf.contrib.rnn.MultiRNNCell([get_lstm_dropout() for _ in range(layer_num)] )\n",
    "initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "outputs, final_state=tf.nn.dynamic_rnn(cell,embed,initial_state=initial_state)\n",
    "\n",
    "#with tf.variable_scope('optmizer'):\n",
    "predictions=tf.contrib.layers.fully_connected(outputs[:,-1],5,activation_fn=tf.nn.softmax)\n",
    "cost=tf.losses.mean_squared_error(label,predictions)\n",
    "optimizer=tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "#validation accuracy\n",
    "correct_pred=tf.equal(tf.cast(tf.round(predictions),tf.int32),label)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batching\n",
    "def get_batching(x,y,batch_size=100):\n",
    "    n_batches=len(x)//batch_size\n",
    "    x,y=x[:n_batches*batch_size],y[:n_batches*batch_size]\n",
    "    for ii in range(0,len(x),batch_size):\n",
    "        yield x[ii:ii+batch_size],y[ii:ii+batch_size]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 100)\n",
      "(80000, 5)\n",
      "(20000, 100)\n",
      "(20000, 5)\n",
      "Epoch: 0/10 Iteration: 50 Train loss: 0.115\n",
      "Epoch: 0/10 Iteration: 100 Train loss: 0.102\n",
      "Val acc: 0.839\n",
      "Epoch: 0/10 Iteration: 150 Train loss: 0.097\n",
      "Epoch: 0/10 Iteration: 200 Train loss: 0.105\n",
      "Val acc: 0.846\n",
      "Epoch: 0/10 Iteration: 250 Train loss: 0.095\n",
      "Epoch: 0/10 Iteration: 300 Train loss: 0.090\n",
      "Val acc: 0.848\n",
      "Epoch: 0/10 Iteration: 350 Train loss: 0.108\n",
      "Epoch: 0/10 Iteration: 400 Train loss: 0.096\n",
      "Val acc: 0.851\n",
      "Epoch: 0/10 Iteration: 450 Train loss: 0.084\n",
      "Epoch: 0/10 Iteration: 500 Train loss: 0.080\n",
      "Val acc: 0.852\n",
      "Epoch: 0/10 Iteration: 550 Train loss: 0.099\n",
      "Epoch: 0/10 Iteration: 600 Train loss: 0.104\n",
      "Val acc: 0.853\n",
      "Epoch: 0/10 Iteration: 650 Train loss: 0.102\n",
      "Epoch: 0/10 Iteration: 700 Train loss: 0.104\n",
      "Val acc: 0.854\n",
      "Epoch: 0/10 Iteration: 750 Train loss: 0.106\n",
      "Epoch: 0/10 Iteration: 800 Train loss: 0.078\n",
      "Val acc: 0.855\n",
      "Epoch: 1/10 Iteration: 850 Train loss: 0.104\n",
      "Epoch: 1/10 Iteration: 900 Train loss: 0.093\n",
      "Val acc: 0.855\n",
      "Epoch: 1/10 Iteration: 950 Train loss: 0.082\n",
      "Epoch: 1/10 Iteration: 1000 Train loss: 0.097\n",
      "Val acc: 0.856\n",
      "Epoch: 1/10 Iteration: 1050 Train loss: 0.086\n",
      "Epoch: 1/10 Iteration: 1100 Train loss: 0.076\n",
      "Val acc: 0.855\n",
      "Epoch: 1/10 Iteration: 1150 Train loss: 0.095\n",
      "Epoch: 1/10 Iteration: 1200 Train loss: 0.083\n",
      "Val acc: 0.856\n",
      "Epoch: 1/10 Iteration: 1250 Train loss: 0.076\n",
      "Epoch: 1/10 Iteration: 1300 Train loss: 0.068\n",
      "Val acc: 0.857\n",
      "Epoch: 1/10 Iteration: 1350 Train loss: 0.091\n",
      "Epoch: 1/10 Iteration: 1400 Train loss: 0.093\n",
      "Val acc: 0.856\n",
      "Epoch: 1/10 Iteration: 1450 Train loss: 0.104\n",
      "Epoch: 1/10 Iteration: 1500 Train loss: 0.102\n",
      "Val acc: 0.857\n",
      "Epoch: 1/10 Iteration: 1550 Train loss: 0.103\n",
      "Epoch: 1/10 Iteration: 1600 Train loss: 0.071\n",
      "Val acc: 0.857\n",
      "Epoch: 2/10 Iteration: 1650 Train loss: 0.097\n",
      "Epoch: 2/10 Iteration: 1700 Train loss: 0.090\n",
      "Val acc: 0.857\n",
      "Epoch: 2/10 Iteration: 1750 Train loss: 0.073\n",
      "Epoch: 2/10 Iteration: 1800 Train loss: 0.089\n",
      "Val acc: 0.858\n",
      "Epoch: 2/10 Iteration: 1850 Train loss: 0.079\n",
      "Epoch: 2/10 Iteration: 1900 Train loss: 0.070\n",
      "Val acc: 0.854\n",
      "Epoch: 2/10 Iteration: 1950 Train loss: 0.092\n",
      "Epoch: 2/10 Iteration: 2000 Train loss: 0.074\n",
      "Val acc: 0.855\n",
      "Epoch: 2/10 Iteration: 2050 Train loss: 0.072\n",
      "Epoch: 2/10 Iteration: 2100 Train loss: 0.060\n",
      "Val acc: 0.855\n",
      "Epoch: 2/10 Iteration: 2150 Train loss: 0.084\n",
      "Epoch: 2/10 Iteration: 2200 Train loss: 0.087\n",
      "Val acc: 0.853\n",
      "Epoch: 2/10 Iteration: 2250 Train loss: 0.098\n",
      "Epoch: 2/10 Iteration: 2300 Train loss: 0.096\n",
      "Val acc: 0.856\n",
      "Epoch: 2/10 Iteration: 2350 Train loss: 0.098\n",
      "Epoch: 2/10 Iteration: 2400 Train loss: 0.068\n",
      "Val acc: 0.856\n",
      "Epoch: 3/10 Iteration: 2450 Train loss: 0.092\n",
      "Epoch: 3/10 Iteration: 2500 Train loss: 0.085\n",
      "Val acc: 0.856\n",
      "Epoch: 3/10 Iteration: 2550 Train loss: 0.065\n",
      "Epoch: 3/10 Iteration: 2600 Train loss: 0.081\n",
      "Val acc: 0.856\n",
      "Epoch: 3/10 Iteration: 2650 Train loss: 0.070\n",
      "Epoch: 3/10 Iteration: 2700 Train loss: 0.066\n",
      "Val acc: 0.852\n",
      "Epoch: 3/10 Iteration: 2750 Train loss: 0.087\n",
      "Epoch: 3/10 Iteration: 2800 Train loss: 0.071\n",
      "Val acc: 0.854\n",
      "Epoch: 3/10 Iteration: 2850 Train loss: 0.062\n",
      "Epoch: 3/10 Iteration: 2900 Train loss: 0.057\n",
      "Val acc: 0.854\n",
      "Epoch: 3/10 Iteration: 2950 Train loss: 0.078\n",
      "Epoch: 3/10 Iteration: 3000 Train loss: 0.082\n",
      "Val acc: 0.851\n",
      "Epoch: 3/10 Iteration: 3050 Train loss: 0.093\n",
      "Epoch: 3/10 Iteration: 3100 Train loss: 0.093\n",
      "Val acc: 0.854\n",
      "Epoch: 3/10 Iteration: 3150 Train loss: 0.089\n",
      "Epoch: 3/10 Iteration: 3200 Train loss: 0.057\n",
      "Val acc: 0.855\n",
      "Epoch: 4/10 Iteration: 3250 Train loss: 0.080\n",
      "Epoch: 4/10 Iteration: 3300 Train loss: 0.081\n",
      "Val acc: 0.855\n",
      "Epoch: 4/10 Iteration: 3350 Train loss: 0.058\n",
      "Epoch: 4/10 Iteration: 3400 Train loss: 0.074\n",
      "Val acc: 0.854\n",
      "Epoch: 4/10 Iteration: 3450 Train loss: 0.060\n",
      "Epoch: 4/10 Iteration: 3500 Train loss: 0.066\n",
      "Val acc: 0.850\n",
      "Epoch: 4/10 Iteration: 3550 Train loss: 0.082\n",
      "Epoch: 4/10 Iteration: 3600 Train loss: 0.063\n",
      "Val acc: 0.853\n",
      "Epoch: 4/10 Iteration: 3650 Train loss: 0.063\n",
      "Epoch: 4/10 Iteration: 3700 Train loss: 0.052\n",
      "Val acc: 0.853\n",
      "Epoch: 4/10 Iteration: 3750 Train loss: 0.076\n",
      "Epoch: 4/10 Iteration: 3800 Train loss: 0.070\n",
      "Val acc: 0.849\n",
      "Epoch: 4/10 Iteration: 3850 Train loss: 0.086\n",
      "Epoch: 4/10 Iteration: 3900 Train loss: 0.083\n",
      "Val acc: 0.851\n",
      "Epoch: 4/10 Iteration: 3950 Train loss: 0.084\n",
      "Epoch: 4/10 Iteration: 4000 Train loss: 0.052\n",
      "Val acc: 0.852\n",
      "Epoch: 5/10 Iteration: 4050 Train loss: 0.074\n",
      "Epoch: 5/10 Iteration: 4100 Train loss: 0.067\n",
      "Val acc: 0.853\n",
      "Epoch: 5/10 Iteration: 4150 Train loss: 0.052\n",
      "Epoch: 5/10 Iteration: 4200 Train loss: 0.064\n",
      "Val acc: 0.853\n",
      "Epoch: 5/10 Iteration: 4250 Train loss: 0.058\n",
      "Epoch: 5/10 Iteration: 4300 Train loss: 0.060\n",
      "Val acc: 0.849\n",
      "Epoch: 5/10 Iteration: 4350 Train loss: 0.068\n",
      "Epoch: 5/10 Iteration: 4400 Train loss: 0.061\n",
      "Val acc: 0.853\n",
      "Epoch: 5/10 Iteration: 4450 Train loss: 0.051\n",
      "Epoch: 5/10 Iteration: 4500 Train loss: 0.049\n",
      "Val acc: 0.852\n",
      "Epoch: 5/10 Iteration: 4550 Train loss: 0.075\n",
      "Epoch: 5/10 Iteration: 4600 Train loss: 0.066\n",
      "Val acc: 0.851\n",
      "Epoch: 5/10 Iteration: 4650 Train loss: 0.078\n",
      "Epoch: 5/10 Iteration: 4700 Train loss: 0.081\n",
      "Val acc: 0.853\n",
      "Epoch: 5/10 Iteration: 4750 Train loss: 0.075\n",
      "Epoch: 5/10 Iteration: 4800 Train loss: 0.053\n",
      "Val acc: 0.851\n",
      "Epoch: 6/10 Iteration: 4850 Train loss: 0.078\n",
      "Epoch: 6/10 Iteration: 4900 Train loss: 0.064\n",
      "Val acc: 0.851\n",
      "Epoch: 6/10 Iteration: 4950 Train loss: 0.039\n",
      "Epoch: 6/10 Iteration: 5000 Train loss: 0.055\n",
      "Val acc: 0.851\n",
      "Epoch: 6/10 Iteration: 5050 Train loss: 0.055\n",
      "Epoch: 6/10 Iteration: 5100 Train loss: 0.046\n",
      "Val acc: 0.848\n",
      "Epoch: 6/10 Iteration: 5150 Train loss: 0.065\n",
      "Epoch: 6/10 Iteration: 5200 Train loss: 0.058\n",
      "Val acc: 0.851\n",
      "Epoch: 6/10 Iteration: 5250 Train loss: 0.046\n",
      "Epoch: 6/10 Iteration: 5300 Train loss: 0.051\n",
      "Val acc: 0.852\n",
      "Epoch: 6/10 Iteration: 5350 Train loss: 0.071\n",
      "Epoch: 6/10 Iteration: 5400 Train loss: 0.061\n",
      "Val acc: 0.849\n",
      "Epoch: 6/10 Iteration: 5450 Train loss: 0.075\n",
      "Epoch: 6/10 Iteration: 5500 Train loss: 0.078\n",
      "Val acc: 0.852\n",
      "Epoch: 6/10 Iteration: 5550 Train loss: 0.067\n",
      "Epoch: 6/10 Iteration: 5600 Train loss: 0.048\n",
      "Val acc: 0.849\n",
      "Epoch: 7/10 Iteration: 5650 Train loss: 0.060\n",
      "Epoch: 7/10 Iteration: 5700 Train loss: 0.056\n",
      "Val acc: 0.851\n",
      "Epoch: 7/10 Iteration: 5750 Train loss: 0.036\n",
      "Epoch: 7/10 Iteration: 5800 Train loss: 0.051\n",
      "Val acc: 0.848\n",
      "Epoch: 7/10 Iteration: 5850 Train loss: 0.043\n",
      "Epoch: 7/10 Iteration: 5900 Train loss: 0.049\n",
      "Val acc: 0.848\n",
      "Epoch: 7/10 Iteration: 5950 Train loss: 0.061\n",
      "Epoch: 7/10 Iteration: 6000 Train loss: 0.052\n",
      "Val acc: 0.851\n",
      "Epoch: 7/10 Iteration: 6050 Train loss: 0.040\n",
      "Epoch: 7/10 Iteration: 6100 Train loss: 0.047\n",
      "Val acc: 0.852\n",
      "Epoch: 7/10 Iteration: 6150 Train loss: 0.060\n",
      "Epoch: 7/10 Iteration: 6200 Train loss: 0.060\n",
      "Val acc: 0.850\n",
      "Epoch: 7/10 Iteration: 6250 Train loss: 0.076\n",
      "Epoch: 7/10 Iteration: 6300 Train loss: 0.073\n",
      "Val acc: 0.850\n",
      "Epoch: 7/10 Iteration: 6350 Train loss: 0.066\n",
      "Epoch: 7/10 Iteration: 6400 Train loss: 0.042\n",
      "Val acc: 0.849\n",
      "Epoch: 8/10 Iteration: 6450 Train loss: 0.061\n",
      "Epoch: 8/10 Iteration: 6500 Train loss: 0.056\n",
      "Val acc: 0.850\n",
      "Epoch: 8/10 Iteration: 6550 Train loss: 0.029\n",
      "Epoch: 8/10 Iteration: 6600 Train loss: 0.051\n",
      "Val acc: 0.851\n",
      "Epoch: 8/10 Iteration: 6650 Train loss: 0.044\n",
      "Epoch: 8/10 Iteration: 6700 Train loss: 0.047\n",
      "Val acc: 0.850\n",
      "Epoch: 8/10 Iteration: 6750 Train loss: 0.054\n",
      "Epoch: 8/10 Iteration: 6800 Train loss: 0.051\n",
      "Val acc: 0.851\n",
      "Epoch: 8/10 Iteration: 6850 Train loss: 0.039\n",
      "Epoch: 8/10 Iteration: 6900 Train loss: 0.039\n",
      "Val acc: 0.852\n",
      "Epoch: 8/10 Iteration: 6950 Train loss: 0.059\n",
      "Epoch: 8/10 Iteration: 7000 Train loss: 0.051\n",
      "Val acc: 0.849\n",
      "Epoch: 8/10 Iteration: 7050 Train loss: 0.065\n",
      "Epoch: 8/10 Iteration: 7100 Train loss: 0.069\n",
      "Val acc: 0.850\n",
      "Epoch: 8/10 Iteration: 7150 Train loss: 0.052\n",
      "Epoch: 8/10 Iteration: 7200 Train loss: 0.041\n",
      "Val acc: 0.849\n",
      "Epoch: 9/10 Iteration: 7250 Train loss: 0.058\n",
      "Epoch: 9/10 Iteration: 7300 Train loss: 0.063\n",
      "Val acc: 0.850\n",
      "Epoch: 9/10 Iteration: 7350 Train loss: 0.029\n",
      "Epoch: 9/10 Iteration: 7400 Train loss: 0.043\n",
      "Val acc: 0.851\n",
      "Epoch: 9/10 Iteration: 7450 Train loss: 0.045\n",
      "Epoch: 9/10 Iteration: 7500 Train loss: 0.038\n",
      "Val acc: 0.851\n",
      "Epoch: 9/10 Iteration: 7550 Train loss: 0.048\n",
      "Epoch: 9/10 Iteration: 7600 Train loss: 0.041\n",
      "Val acc: 0.850\n",
      "Epoch: 9/10 Iteration: 7650 Train loss: 0.037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10 Iteration: 7700 Train loss: 0.040\n",
      "Val acc: 0.852\n",
      "Epoch: 9/10 Iteration: 7750 Train loss: 0.055\n",
      "Epoch: 9/10 Iteration: 7800 Train loss: 0.048\n",
      "Val acc: 0.850\n",
      "Epoch: 9/10 Iteration: 7850 Train loss: 0.065\n",
      "Epoch: 9/10 Iteration: 7900 Train loss: 0.063\n",
      "Val acc: 0.849\n",
      "Epoch: 9/10 Iteration: 7950 Train loss: 0.050\n",
      "Epoch: 9/10 Iteration: 8000 Train loss: 0.046\n",
      "Val acc: 0.849\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "#super params\n",
    "epochs=10\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session()as sess:\n",
    "    writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_train, x_test, y_train, y_test=train_test_split(combined, Y, test_size=0.2)#get_data(index_dict,word_vectors,combined,Y)\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(x_test.shape)\n",
    "    print(y_test.shape)\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        for ii, (x, y) in enumerate(get_batching(x_train, y_train, batch_size), 1):\n",
    "            feed={input_x:x,label:y,lr:0.001,keep_prob:0.5}\n",
    "            loss,_=sess.run([cost,optimizer],feed_dict=feed)\n",
    "            tf.summary.scalar('loss',loss)\n",
    "            if iteration%50==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "            if iteration%100==0:\n",
    "                val_acc = []\n",
    "                for x, y in get_batching(x_test, y_test, batch_size):\n",
    "                    feed = {input_x: x,\n",
    "                            label: y,                            \n",
    "                            keep_prob: 1}\n",
    "                    batch_acc= sess.run(accuracy, feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                tf.summary.scalar('accuracy',np.mean(val_acc))\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n",
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "predict_data=pd.read_csv('data/predict_first.csv')\n",
    "cabs=[cut(x) for x in predict_data['Discuss']]\n",
    "_,_,predict_X=word2vec_train(cabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "model_file=tf.train.latest_checkpoint('checkpoints/')\n",
    "with tf.Session()as sess:\n",
    "    #sess.run(tf.global_variables_initializer()) \n",
    "    saver.restore(sess,model_file)\n",
    "    predict_arr=[]\n",
    "    for ii in range(0,len(predict_X),100):\n",
    "        for p in sess.run(predictions,feed_dict={input_x:predict_X[ii:ii+100],keep_prob:1.0}):\n",
    "            predict_arr.append(np.argmax(p)+1)\n",
    "    #print(sess.run(predictions,feed_dict={input_x:predict_X[0:100],keep_prob:1.}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
